{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dbeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一导入工具包\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91697b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torchtext.data.Field 声明处理数据的方式\n",
    "参数说明：\n",
    "    tokenize 分词处理\n",
    "    init_token 定义开始字符\n",
    "    eos_token 定义结束字符\n",
    "    lower 小写化处理\n",
    "'''\n",
    "# 声明处理方式，主要包括分词和小写化处理\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wikitext-2-v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "datasets\\wikitext-2\\wikitext-2-v1.zip: 100%|███████████████████████████████████████| 4.48M/4.48M [00:07<00:00, 571kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n",
      "train_txt tokens: 2086708\n",
      "val_txt tokens:   218177\n",
      "test_txt tokens:  246217\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT, root='./datasets', train='wiki.train.tokens', validation='wiki.valid.tokens', test='wiki.test.tokens')\n",
    "\n",
    "# 统计数据\n",
    "print('train_txt tokens: %d' % len(train_txt.examples[0].text))\n",
    "print('val_txt tokens:   %d' % len(val_txt.examples[0].text))\n",
    "print('test_txt tokens:  %d' % len(test_txt.examples[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b67659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 28785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x000001BD2B2002B0>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<sos>': 2,\n",
       "             '<eos>': 3,\n",
       "             'the': 4,\n",
       "             ',': 5,\n",
       "             '.': 6,\n",
       "             'of': 7,\n",
       "             'and': 8,\n",
       "             'in': 9,\n",
       "             'to': 10,\n",
       "             'a': 11,\n",
       "             '=': 12,\n",
       "             'was': 13,\n",
       "             \"'\": 14,\n",
       "             '@-@': 15,\n",
       "             'on': 16,\n",
       "             'as': 17,\n",
       "             's': 18,\n",
       "             'that': 19,\n",
       "             'for': 20,\n",
       "             'with': 21,\n",
       "             'by': 22,\n",
       "             ')': 23,\n",
       "             '(': 24,\n",
       "             '@': 25,\n",
       "             'is': 26,\n",
       "             'it': 27,\n",
       "             'from': 28,\n",
       "             'at': 29,\n",
       "             'his': 30,\n",
       "             'he': 31,\n",
       "             'were': 32,\n",
       "             'an': 33,\n",
       "             'had': 34,\n",
       "             'which': 35,\n",
       "             'be': 36,\n",
       "             'are': 37,\n",
       "             'this': 38,\n",
       "             'their': 39,\n",
       "             'first': 40,\n",
       "             'but': 41,\n",
       "             'not': 42,\n",
       "             '–': 43,\n",
       "             'one': 44,\n",
       "             'they': 45,\n",
       "             'its': 46,\n",
       "             'also': 47,\n",
       "             'after': 48,\n",
       "             'her': 49,\n",
       "             'or': 50,\n",
       "             'two': 51,\n",
       "             'have': 52,\n",
       "             'has': 53,\n",
       "             'been': 54,\n",
       "             'who': 55,\n",
       "             'she': 56,\n",
       "             'new': 57,\n",
       "             'other': 58,\n",
       "             'during': 59,\n",
       "             'when': 60,\n",
       "             'time': 61,\n",
       "             'all': 62,\n",
       "             'into': 63,\n",
       "             'more': 64,\n",
       "             'would': 65,\n",
       "             '1': 66,\n",
       "             'i': 67,\n",
       "             'over': 68,\n",
       "             'while': 69,\n",
       "             'game': 70,\n",
       "             'only': 71,\n",
       "             'most': 72,\n",
       "             '2': 73,\n",
       "             'three': 74,\n",
       "             'later': 75,\n",
       "             'about': 76,\n",
       "             'up': 77,\n",
       "             'may': 78,\n",
       "             'between': 79,\n",
       "             'him': 80,\n",
       "             'song': 81,\n",
       "             'there': 82,\n",
       "             'some': 83,\n",
       "             'than': 84,\n",
       "             'out': 85,\n",
       "             'no': 86,\n",
       "             'season': 87,\n",
       "             'year': 88,\n",
       "             'made': 89,\n",
       "             'city': 90,\n",
       "             '3': 91,\n",
       "             'such': 92,\n",
       "             'before': 93,\n",
       "             'where': 94,\n",
       "             'used': 95,\n",
       "             'series': 96,\n",
       "             'them': 97,\n",
       "             'second': 98,\n",
       "             'world': 99,\n",
       "             'being': 100,\n",
       "             'years': 101,\n",
       "             'both': 102,\n",
       "             '000': 103,\n",
       "             'many': 104,\n",
       "             'these': 105,\n",
       "             'film': 106,\n",
       "             'however': 107,\n",
       "             'album': 108,\n",
       "             'south': 109,\n",
       "             'war': 110,\n",
       "             'through': 111,\n",
       "             '5': 112,\n",
       "             'north': 113,\n",
       "             'then': 114,\n",
       "             'can': 115,\n",
       "             'part': 116,\n",
       "             'early': 117,\n",
       "             'several': 118,\n",
       "             '4': 119,\n",
       "             'number': 120,\n",
       "             'state': 121,\n",
       "             'including': 122,\n",
       "             'against': 123,\n",
       "             'well': 124,\n",
       "             '/': 125,\n",
       "             'known': 126,\n",
       "             'became': 127,\n",
       "             '—': 128,\n",
       "             'm': 129,\n",
       "             'four': 130,\n",
       "             'united': 131,\n",
       "             'under': 132,\n",
       "             'although': 133,\n",
       "             'century': 134,\n",
       "             'day': 135,\n",
       "             'following': 136,\n",
       "             'music': 137,\n",
       "             'began': 138,\n",
       "             'because': 139,\n",
       "             'so': 140,\n",
       "             'work': 141,\n",
       "             'like': 142,\n",
       "             'end': 143,\n",
       "             'called': 144,\n",
       "             'episode': 145,\n",
       "             'until': 146,\n",
       "             'found': 147,\n",
       "             'said': 148,\n",
       "             'area': 149,\n",
       "             'could': 150,\n",
       "             'states': 151,\n",
       "             'american': 152,\n",
       "             'people': 153,\n",
       "             '6': 154,\n",
       "             'since': 155,\n",
       "             'british': 156,\n",
       "             'each': 157,\n",
       "             'released': 158,\n",
       "             'same': 159,\n",
       "             'team': 160,\n",
       "             'church': 161,\n",
       "             '10': 162,\n",
       "             'around': 163,\n",
       "             'long': 164,\n",
       "             'did': 165,\n",
       "             'along': 166,\n",
       "             'million': 167,\n",
       "             'five': 168,\n",
       "             'life': 169,\n",
       "             'national': 170,\n",
       "             '0': 171,\n",
       "             'back': 172,\n",
       "             'john': 173,\n",
       "             'high': 174,\n",
       "             'company': 175,\n",
       "             't': 176,\n",
       "             'another': 177,\n",
       "             'best': 178,\n",
       "             'use': 179,\n",
       "             '%': 180,\n",
       "             'you': 181,\n",
       "             'if': 182,\n",
       "             'final': 183,\n",
       "             'september': 184,\n",
       "             'august': 185,\n",
       "             'river': 186,\n",
       "             'large': 187,\n",
       "             'what': 188,\n",
       "             'west': 189,\n",
       "             '8': 190,\n",
       "             'km': 191,\n",
       "             'off': 192,\n",
       "             'down': 193,\n",
       "             '7': 194,\n",
       "             'due': 195,\n",
       "             'games': 196,\n",
       "             'june': 197,\n",
       "             'line': 198,\n",
       "             'history': 199,\n",
       "             'will': 200,\n",
       "             'name': 201,\n",
       "             'now': 202,\n",
       "             'any': 203,\n",
       "             'storm': 204,\n",
       "             'home': 205,\n",
       "             'received': 206,\n",
       "             '9': 207,\n",
       "             'described': 208,\n",
       "             'government': 209,\n",
       "             'six': 210,\n",
       "             'species': 211,\n",
       "             'within': 212,\n",
       "             'much': 213,\n",
       "             'group': 214,\n",
       "             'family': 215,\n",
       "             'october': 216,\n",
       "             'played': 217,\n",
       "             '$': 218,\n",
       "             'east': 219,\n",
       "             'league': 220,\n",
       "             'general': 221,\n",
       "             'set': 222,\n",
       "             'took': 223,\n",
       "             '[': 224,\n",
       "             ']': 225,\n",
       "             'major': 226,\n",
       "             'road': 227,\n",
       "             'july': 228,\n",
       "             'wrote': 229,\n",
       "             'late': 230,\n",
       "             'single': 231,\n",
       "             'won': 232,\n",
       "             'system': 233,\n",
       "             'play': 234,\n",
       "             'video': 235,\n",
       "             'times': 236,\n",
       "             'us': 237,\n",
       "             'according': 238,\n",
       "             'record': 239,\n",
       "             'third': 240,\n",
       "             'based': 241,\n",
       "             'april': 242,\n",
       "             'man': 243,\n",
       "             'included': 244,\n",
       "             'just': 245,\n",
       "             'march': 246,\n",
       "             'book': 247,\n",
       "             'those': 248,\n",
       "             'january': 249,\n",
       "             'show': 250,\n",
       "             'named': 251,\n",
       "             'even': 252,\n",
       "             'very': 253,\n",
       "             'england': 254,\n",
       "             'main': 255,\n",
       "             'white': 256,\n",
       "             'left': 257,\n",
       "             'york': 258,\n",
       "             'men': 259,\n",
       "             'school': 260,\n",
       "             'small': 261,\n",
       "             'though': 262,\n",
       "             'division': 263,\n",
       "             'club': 264,\n",
       "             'way': 265,\n",
       "             'old': 266,\n",
       "             'original': 267,\n",
       "             'near': 268,\n",
       "             'last': 269,\n",
       "             '12': 270,\n",
       "             'november': 271,\n",
       "             'water': 272,\n",
       "             'death': 273,\n",
       "             'place': 274,\n",
       "             '20': 275,\n",
       "             '15': 276,\n",
       "             'tropical': 277,\n",
       "             'december': 278,\n",
       "             'built': 279,\n",
       "             'own': 280,\n",
       "             'character': 281,\n",
       "             'we': 282,\n",
       "             'songs': 283,\n",
       "             'top': 284,\n",
       "             'de': 285,\n",
       "             'form': 286,\n",
       "             '30': 287,\n",
       "             'player': 288,\n",
       "             'do': 289,\n",
       "             'king': 290,\n",
       "             'black': 291,\n",
       "             'public': 292,\n",
       "             'german': 293,\n",
       "             'island': 294,\n",
       "             'next': 295,\n",
       "             '2009': 296,\n",
       "             'make': 297,\n",
       "             '2008': 298,\n",
       "             'still': 299,\n",
       "             '2010': 300,\n",
       "             'u': 301,\n",
       "             'role': 302,\n",
       "             'led': 303,\n",
       "             'again': 304,\n",
       "             'moved': 305,\n",
       "             'career': 306,\n",
       "             'ii': 307,\n",
       "             'university': 308,\n",
       "             'without': 309,\n",
       "             'love': 310,\n",
       "             'often': 311,\n",
       "             'among': 312,\n",
       "             'recorded': 313,\n",
       "             'further': 314,\n",
       "             'hurricane': 315,\n",
       "             'military': 316,\n",
       "             'period': 317,\n",
       "             'star': 318,\n",
       "             'local': 319,\n",
       "             'considered': 320,\n",
       "             'army': 321,\n",
       "             'production': 322,\n",
       "             'release': 323,\n",
       "             'side': 324,\n",
       "             '2007': 325,\n",
       "             'great': 326,\n",
       "             'house': 327,\n",
       "             'came': 328,\n",
       "             'published': 329,\n",
       "             'written': 330,\n",
       "             '100': 331,\n",
       "             'continued': 332,\n",
       "             'power': 333,\n",
       "             'town': 334,\n",
       "             'english': 335,\n",
       "             'story': 336,\n",
       "             'days': 337,\n",
       "             'forces': 338,\n",
       "             'run': 339,\n",
       "             'held': 340,\n",
       "             'route': 341,\n",
       "             'french': 342,\n",
       "             'support': 343,\n",
       "             '14': 344,\n",
       "             '16': 345,\n",
       "             '11': 346,\n",
       "             'force': 347,\n",
       "             'half': 348,\n",
       "             'few': 349,\n",
       "             'take': 350,\n",
       "             'international': 351,\n",
       "             'having': 352,\n",
       "             '25': 353,\n",
       "             'county': 354,\n",
       "             'land': 355,\n",
       "             'throughout': 356,\n",
       "             '2011': 357,\n",
       "             'point': 358,\n",
       "             '18': 359,\n",
       "             'become': 360,\n",
       "             '2006': 361,\n",
       "             'children': 362,\n",
       "             'order': 363,\n",
       "             'light': 364,\n",
       "             'version': 365,\n",
       "             'title': 366,\n",
       "             'former': 367,\n",
       "             'lost': 368,\n",
       "             'track': 369,\n",
       "             'different': 370,\n",
       "             '&': 371,\n",
       "             'development': 372,\n",
       "             'field': 373,\n",
       "             'ship': 374,\n",
       "             'similar': 375,\n",
       "             'despite': 376,\n",
       "             'live': 377,\n",
       "             'common': 378,\n",
       "             'members': 379,\n",
       "             'park': 380,\n",
       "             'c': 381,\n",
       "             'february': 382,\n",
       "             '13': 383,\n",
       "             'gave': 384,\n",
       "             'produced': 385,\n",
       "             'short': 386,\n",
       "             'southern': 387,\n",
       "             '!': 388,\n",
       "             'dylan': 389,\n",
       "             'little': 390,\n",
       "             'site': 391,\n",
       "             '2012': 392,\n",
       "             'once': 393,\n",
       "             'television': 394,\n",
       "             'writing': 395,\n",
       "             'given': 396,\n",
       "             'central': 397,\n",
       "             'control': 398,\n",
       "             'total': 399,\n",
       "             'band': 400,\n",
       "             'country': 401,\n",
       "             'service': 402,\n",
       "             'northern': 403,\n",
       "             're': 404,\n",
       "             'include': 405,\n",
       "             'young': 406,\n",
       "             'fire': 407,\n",
       "             'position': 408,\n",
       "             'battalion': 409,\n",
       "             'making': 410,\n",
       "             'never': 411,\n",
       "             'away': 412,\n",
       "             'seven': 413,\n",
       "             'tour': 414,\n",
       "             'age': 415,\n",
       "             'air': 416,\n",
       "             'lead': 417,\n",
       "             '2013': 418,\n",
       "             'how': 419,\n",
       "             'open': 420,\n",
       "             'reported': 421,\n",
       "             'seen': 422,\n",
       "             'battle': 423,\n",
       "             'highway': 424,\n",
       "             'eastern': 425,\n",
       "             'good': 426,\n",
       "             'western': 427,\n",
       "             'stated': 428,\n",
       "             'attack': 429,\n",
       "             'red': 430,\n",
       "             'god': 431,\n",
       "             'h': 432,\n",
       "             'match': 433,\n",
       "             'across': 434,\n",
       "             'st': 435,\n",
       "             'body': 436,\n",
       "             'instead': 437,\n",
       "             'returned': 438,\n",
       "             'ships': 439,\n",
       "             'established': 440,\n",
       "             'using': 441,\n",
       "             'ft': 442,\n",
       "             'population': 443,\n",
       "             'america': 444,\n",
       "             'construction': 445,\n",
       "             'modern': 446,\n",
       "             'week': 447,\n",
       "             'noted': 448,\n",
       "             'less': 449,\n",
       "             'my': 450,\n",
       "             'royal': 451,\n",
       "             'head': 452,\n",
       "             'reached': 453,\n",
       "             'building': 454,\n",
       "             'developed': 455,\n",
       "             'eight': 456,\n",
       "             'rock': 457,\n",
       "             'ireland': 458,\n",
       "             'players': 459,\n",
       "             'brigade': 460,\n",
       "             'b': 461,\n",
       "             'president': 462,\n",
       "             'result': 463,\n",
       "             'thought': 464,\n",
       "             'performance': 465,\n",
       "             'right': 466,\n",
       "             'london': 467,\n",
       "             'miles': 468,\n",
       "             'himself': 469,\n",
       "             'father': 470,\n",
       "             'per': 471,\n",
       "             'important': 472,\n",
       "             'style': 473,\n",
       "             'performed': 474,\n",
       "             'felt': 475,\n",
       "             'australia': 476,\n",
       "             'various': 477,\n",
       "             '17': 478,\n",
       "             'full': 479,\n",
       "             'areas': 480,\n",
       "             'feet': 481,\n",
       "             'previous': 482,\n",
       "             'events': 483,\n",
       "             'win': 484,\n",
       "             'low': 485,\n",
       "             'died': 486,\n",
       "             'kingdom': 487,\n",
       "             'guitar': 488,\n",
       "             'football': 489,\n",
       "             'others': 490,\n",
       "             'art': 491,\n",
       "             'mm': 492,\n",
       "             'originally': 493,\n",
       "             'project': 494,\n",
       "             'too': 495,\n",
       "             'went': 496,\n",
       "             'human': 497,\n",
       "             '23': 498,\n",
       "             'level': 499,\n",
       "             'upon': 500,\n",
       "             'range': 501,\n",
       "             'works': 502,\n",
       "             'formed': 503,\n",
       "             'started': 504,\n",
       "             'characters': 505,\n",
       "             'james': 506,\n",
       "             'political': 507,\n",
       "             'women': 508,\n",
       "             'should': 509,\n",
       "             'cup': 510,\n",
       "             'port': 511,\n",
       "             '50': 512,\n",
       "             'caused': 513,\n",
       "             '21': 514,\n",
       "             '28': 515,\n",
       "             'eventually': 516,\n",
       "             'located': 517,\n",
       "             '19': 518,\n",
       "             '24': 519,\n",
       "             'stars': 520,\n",
       "             'critics': 521,\n",
       "             'ground': 522,\n",
       "             'sent': 523,\n",
       "             'able': 524,\n",
       "             'created': 525,\n",
       "             '2004': 526,\n",
       "             'me': 527,\n",
       "             '2005': 528,\n",
       "             'class': 529,\n",
       "             'd': 530,\n",
       "             'chart': 531,\n",
       "             'night': 532,\n",
       "             'born': 533,\n",
       "             'region': 534,\n",
       "             'street': 535,\n",
       "             'center': 536,\n",
       "             'court': 537,\n",
       "             'design': 538,\n",
       "             'together': 539,\n",
       "             'director': 540,\n",
       "             'popular': 541,\n",
       "             'present': 542,\n",
       "             'strong': 543,\n",
       "             'award': 544,\n",
       "             'every': 545,\n",
       "             '’': 546,\n",
       "             'return': 547,\n",
       "             'son': 548,\n",
       "             'hero': 549,\n",
       "             'remained': 550,\n",
       "             'see': 551,\n",
       "             'completed': 552,\n",
       "             'guns': 553,\n",
       "             'novel': 554,\n",
       "             'scored': 555,\n",
       "             'announced': 556,\n",
       "             'australian': 557,\n",
       "             'grand': 558,\n",
       "             '22': 559,\n",
       "             'almost': 560,\n",
       "             'fourth': 561,\n",
       "             'behind': 562,\n",
       "             'damage': 563,\n",
       "             'least': 564,\n",
       "             '26': 565,\n",
       "             'brown': 566,\n",
       "             'party': 567,\n",
       "             'ten': 568,\n",
       "             'added': 569,\n",
       "             'heavy': 570,\n",
       "             'followed': 571,\n",
       "             'months': 572,\n",
       "             'appeared': 573,\n",
       "             'wife': 574,\n",
       "             'killed': 575,\n",
       "             'addition': 576,\n",
       "             'does': 577,\n",
       "             'playing': 578,\n",
       "             'success': 579,\n",
       "             'awards': 580,\n",
       "             'list': 581,\n",
       "             'features': 582,\n",
       "             'aircraft': 583,\n",
       "             '2003': 584,\n",
       "             'coast': 585,\n",
       "             'sea': 586,\n",
       "             'taken': 587,\n",
       "             '2015': 588,\n",
       "             'david': 589,\n",
       "             'leading': 590,\n",
       "             'championship': 591,\n",
       "             'action': 592,\n",
       "             'france': 593,\n",
       "             'either': 594,\n",
       "             'europe': 595,\n",
       "             'front': 596,\n",
       "             'recording': 597,\n",
       "             'served': 598,\n",
       "             'towards': 599,\n",
       "             'campaign': 600,\n",
       "             'operations': 601,\n",
       "             'gold': 602,\n",
       "             'mother': 603,\n",
       "             'put': 604,\n",
       "             'decided': 605,\n",
       "             'elements': 606,\n",
       "             'close': 607,\n",
       "             'records': 608,\n",
       "             'believed': 609,\n",
       "             'fleet': 610,\n",
       "             'generally': 611,\n",
       "             'magazine': 612,\n",
       "             'carey': 613,\n",
       "             'ever': 614,\n",
       "             'female': 615,\n",
       "             'post': 616,\n",
       "             'poem': 617,\n",
       "             'sold': 618,\n",
       "             'soon': 619,\n",
       "             'example': 620,\n",
       "             'infantry': 621,\n",
       "             'points': 622,\n",
       "             'significant': 623,\n",
       "             'fort': 624,\n",
       "             'goal': 625,\n",
       "             'move': 626,\n",
       "             'weeks': 627,\n",
       "             'rather': 628,\n",
       "             'study': 629,\n",
       "             'european': 630,\n",
       "             'federer': 631,\n",
       "             'outside': 632,\n",
       "             'o': 633,\n",
       "             'opened': 634,\n",
       "             'robert': 635,\n",
       "             'case': 636,\n",
       "             'directed': 637,\n",
       "             'brought': 638,\n",
       "             'help': 639,\n",
       "             'law': 640,\n",
       "             'non': 641,\n",
       "             'finished': 642,\n",
       "             '27': 643,\n",
       "             'earlier': 644,\n",
       "             'wales': 645,\n",
       "             'william': 646,\n",
       "             'featured': 647,\n",
       "             'go': 648,\n",
       "             'get': 649,\n",
       "             'victory': 650,\n",
       "             'manager': 651,\n",
       "             'successful': 652,\n",
       "             'act': 653,\n",
       "             'gun': 654,\n",
       "             'stage': 655,\n",
       "             'association': 656,\n",
       "             'member': 657,\n",
       "             'provided': 658,\n",
       "             'mi': 659,\n",
       "             'mid': 660,\n",
       "             'opening': 661,\n",
       "             'start': 662,\n",
       "             'village': 663,\n",
       "             'working': 664,\n",
       "             'council': 665,\n",
       "             'wanted': 666,\n",
       "             'appearance': 667,\n",
       "             'jordan': 668,\n",
       "             'particularly': 669,\n",
       "             'roman': 670,\n",
       "             'troops': 671,\n",
       "             '2014': 672,\n",
       "             '40': 673,\n",
       "             'atlantic': 674,\n",
       "             'depression': 675,\n",
       "             'initially': 676,\n",
       "             'tech': 677,\n",
       "             '29': 678,\n",
       "             'evidence': 679,\n",
       "             'yard': 680,\n",
       "             'far': 681,\n",
       "             'find': 682,\n",
       "             'largest': 683,\n",
       "             'office': 684,\n",
       "             'blue': 685,\n",
       "             'dam': 686,\n",
       "             'george': 687,\n",
       "             'review': 688,\n",
       "             'attempt': 689,\n",
       "             'possible': 690,\n",
       "             'saw': 691,\n",
       "             'special': 692,\n",
       "             'type': 693,\n",
       "             'month': 694,\n",
       "             'summer': 695,\n",
       "             '19th': 696,\n",
       "             'above': 697,\n",
       "             'union': 698,\n",
       "             'yards': 699,\n",
       "             'e': 700,\n",
       "             'florida': 701,\n",
       "             'rest': 702,\n",
       "             'allowed': 703,\n",
       "             'event': 704,\n",
       "             'race': 705,\n",
       "             'winds': 706,\n",
       "             'critical': 707,\n",
       "             'saying': 708,\n",
       "             'creek': 709,\n",
       "             'cross': 710,\n",
       "             'hours': 711,\n",
       "             'whom': 712,\n",
       "             'nine': 713,\n",
       "             '2001': 714,\n",
       "             'missouri': 715,\n",
       "             'plan': 716,\n",
       "             'police': 717,\n",
       "             'worked': 718,\n",
       "             'community': 719,\n",
       "             'designed': 720,\n",
       "             'reception': 721,\n",
       "             'society': 722,\n",
       "             '500': 723,\n",
       "             'previously': 724,\n",
       "             'free': 725,\n",
       "             'forced': 726,\n",
       "             'middle': 727,\n",
       "             'process': 728,\n",
       "             'era': 729,\n",
       "             'operation': 730,\n",
       "             'radio': 731,\n",
       "             'real': 732,\n",
       "             'remains': 733,\n",
       "             '200': 734,\n",
       "             'increased': 735,\n",
       "             'official': 736,\n",
       "             'praised': 737,\n",
       "             'research': 738,\n",
       "             'hall': 739,\n",
       "             'lower': 740,\n",
       "             'parliament': 741,\n",
       "             'station': 742,\n",
       "             'come': 743,\n",
       "             'michael': 744,\n",
       "             'relationship': 745,\n",
       "             'command': 746,\n",
       "             'commander': 747,\n",
       "             'hill': 748,\n",
       "             'regiment': 749,\n",
       "             'studio': 750,\n",
       "             'units': 751,\n",
       "             'base': 752,\n",
       "             'taking': 753,\n",
       "             '£': 754,\n",
       "             'parts': 755,\n",
       "             'replaced': 756,\n",
       "             'writer': 757,\n",
       "             'ball': 758,\n",
       "             'industry': 759,\n",
       "             'media': 760,\n",
       "             'navy': 761,\n",
       "             'social': 762,\n",
       "             'food': 763,\n",
       "             'r': 764,\n",
       "             'bay': 765,\n",
       "             'co': 766,\n",
       "             'college': 767,\n",
       "             'highest': 768,\n",
       "             'reviews': 769,\n",
       "             'beginning': 770,\n",
       "             'claimed': 771,\n",
       "             'don': 772,\n",
       "             '°': 773,\n",
       "             'canada': 774,\n",
       "             'estimated': 775,\n",
       "             'mph': 776,\n",
       "             'museum': 777,\n",
       "             'section': 778,\n",
       "             'goals': 779,\n",
       "             'stone': 780,\n",
       "             'average': 781,\n",
       "             'commercial': 782,\n",
       "             'japanese': 783,\n",
       "             'joined': 784,\n",
       "             'l': 785,\n",
       "             'religious': 786,\n",
       "             'involved': 787,\n",
       "             'oldham': 788,\n",
       "             'placed': 789,\n",
       "             'stories': 790,\n",
       "             'training': 791,\n",
       "             'introduced': 792,\n",
       "             'met': 793,\n",
       "             'shot': 794,\n",
       "             'signed': 795,\n",
       "             'suggested': 796,\n",
       "             'lines': 797,\n",
       "             'sometimes': 798,\n",
       "             '31': 799,\n",
       "             'background': 800,\n",
       "             'business': 801,\n",
       "             'face': 802,\n",
       "             'olivier': 803,\n",
       "             'paul': 804,\n",
       "             'today': 805,\n",
       "             'complete': 806,\n",
       "             'going': 807,\n",
       "             'itself': 808,\n",
       "             'scene': 809,\n",
       "             'henry': 810,\n",
       "             'mexico': 811,\n",
       "             'structure': 812,\n",
       "             'additional': 813,\n",
       "             'available': 814,\n",
       "             'give': 815,\n",
       "             'thus': 816,\n",
       "             'cast': 817,\n",
       "             'date': 818,\n",
       "             'horse': 819,\n",
       "             'language': 820,\n",
       "             'loss': 821,\n",
       "             'india': 822,\n",
       "             'nearly': 823,\n",
       "             'sound': 824,\n",
       "             'term': 825,\n",
       "             'whose': 826,\n",
       "             'fifth': 827,\n",
       "             'past': 828,\n",
       "             'thomas': 829,\n",
       "             'approximately': 830,\n",
       "             'indian': 831,\n",
       "             'irish': 832,\n",
       "             'must': 833,\n",
       "             'program': 834,\n",
       "             'already': 835,\n",
       "             'appointed': 836,\n",
       "             'capital': 837,\n",
       "             'entire': 838,\n",
       "             'friends': 839,\n",
       "             'britain': 840,\n",
       "             'damaged': 841,\n",
       "             'native': 842,\n",
       "             'prior': 843,\n",
       "             'shows': 844,\n",
       "             'told': 845,\n",
       "             'forest': 846,\n",
       "             'issue': 847,\n",
       "             'mark': 848,\n",
       "             'names': 849,\n",
       "             'probably': 850,\n",
       "             'turned': 851,\n",
       "             'male': 852,\n",
       "             'sun': 853,\n",
       "             'winning': 854,\n",
       "             '-': 855,\n",
       "             'change': 856,\n",
       "             'g': 857,\n",
       "             'our': 858,\n",
       "             'students': 859,\n",
       "             '?': 860,\n",
       "             'earth': 861,\n",
       "             'length': 862,\n",
       "             'passed': 863,\n",
       "             'size': 864,\n",
       "             'child': 865,\n",
       "             'civil': 866,\n",
       "             'especially': 867,\n",
       "             'christian': 868,\n",
       "             'enough': 869,\n",
       "             'notes': 870,\n",
       "             'woman': 871,\n",
       "             'chinese': 872,\n",
       "             'failed': 873,\n",
       "             'forward': 874,\n",
       "             'mixed': 875,\n",
       "             'overall': 876,\n",
       "             'running': 877,\n",
       "             'better': 878,\n",
       "             'captain': 879,\n",
       "             'ny': 880,\n",
       "             '2002': 881,\n",
       "             'limited': 882,\n",
       "             'future': 883,\n",
       "             'iii': 884,\n",
       "             'minor': 885,\n",
       "             'network': 886,\n",
       "             'arrived': 887,\n",
       "             'changes': 888,\n",
       "             'includes': 889,\n",
       "             'might': 890,\n",
       "             'moving': 891,\n",
       "             'ordered': 892,\n",
       "             'pacific': 893,\n",
       "             'regular': 894,\n",
       "             'spent': 895,\n",
       "             'wheeler': 896,\n",
       "             'canadian': 897,\n",
       "             'cathedral': 898,\n",
       "             'education': 899,\n",
       "             'larger': 900,\n",
       "             'remaining': 901,\n",
       "             'usually': 902,\n",
       "             'birds': 903,\n",
       "             'department': 904,\n",
       "             'hand': 905,\n",
       "             'hit': 906,\n",
       "             'lake': 907,\n",
       "             'required': 908,\n",
       "             'san': 909,\n",
       "             'uk': 910,\n",
       "             'decision': 911,\n",
       "             'latter': 912,\n",
       "             'africa': 913,\n",
       "             'plot': 914,\n",
       "             'response': 915,\n",
       "             '2000': 916,\n",
       "             'musical': 917,\n",
       "             'round': 918,\n",
       "             'space': 919,\n",
       "             'voice': 920,\n",
       "             'wide': 921,\n",
       "             'appear': 922,\n",
       "             'crew': 923,\n",
       "             'debut': 924,\n",
       "             'groups': 925,\n",
       "             'mounted': 926,\n",
       "             'related': 927,\n",
       "             'centre': 928,\n",
       "             'jin': 929,\n",
       "             'rachel': 930,\n",
       "             'territory': 931,\n",
       "             'view': 932,\n",
       "             '00': 933,\n",
       "             'billboard': 934,\n",
       "             'ended': 935,\n",
       "             'feature': 936,\n",
       "             'films': 937,\n",
       "             'nature': 938,\n",
       "             'positive': 939,\n",
       "             'saint': 940,\n",
       "             'science': 941,\n",
       "             'culture': 942,\n",
       "             'finally': 943,\n",
       "             'flight': 944,\n",
       "             'score': 945,\n",
       "             'squadron': 946,\n",
       "             'supported': 947,\n",
       "             'becoming': 948,\n",
       "             'money': 949,\n",
       "             'pressure': 950,\n",
       "             'always': 951,\n",
       "             'books': 952,\n",
       "             'charles': 953,\n",
       "             'provide': 954,\n",
       "             'smaller': 955,\n",
       "             '1995': 956,\n",
       "             'anti': 957,\n",
       "             'discovered': 958,\n",
       "             'private': 959,\n",
       "             'shown': 960,\n",
       "             'board': 961,\n",
       "             'minutes': 962,\n",
       "             'particular': 963,\n",
       "             'shortly': 964,\n",
       "             'defeated': 965,\n",
       "             'difficult': 966,\n",
       "             'experience': 967,\n",
       "             'mass': 968,\n",
       "             'nations': 969,\n",
       "             'person': 970,\n",
       "             'peter': 971,\n",
       "             'temple': 972,\n",
       "             'trade': 973,\n",
       "             'big': 974,\n",
       "             'staff': 975,\n",
       "             'subsequently': 976,\n",
       "             'surface': 977,\n",
       "             'effects': 978,\n",
       "             'japan': 979,\n",
       "             'lack': 980,\n",
       "             'living': 981,\n",
       "             'press': 982,\n",
       "             'upper': 983,\n",
       "             'zealand': 984,\n",
       "             'professional': 985,\n",
       "             'word': 986,\n",
       "             'fact': 987,\n",
       "             'greater': 988,\n",
       "             'material': 989,\n",
       "             'tv': 990,\n",
       "             '60': 991,\n",
       "             'problems': 992,\n",
       "             'room': 993,\n",
       "             'self': 994,\n",
       "             'teams': 995,\n",
       "             'bridge': 996,\n",
       "             'collection': 997,\n",
       "             'cut': 998,\n",
       "             'gods': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依据训练集构建词典\n",
    "TEXT.build_vocab(train_txt)\n",
    "\n",
    "# 查看词典\n",
    "length = len(TEXT.vocab)\n",
    "print('词表大小: %d' % length)\n",
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bfe48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    '''\n",
    "    将数据划分为用于训练的批次\n",
    "    '''\n",
    "    # 将文本形式的数据用 token 的相应索引来表示\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # 获取总的批次数目\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # 去除剩余的部分，比如总长度为12，而批次大小为5，那么剩余的2个 token 将不会被包括在内\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # 根据批次大小，划分数据集\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a2ddb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104335, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d26d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35 # 句子长度\n",
    "def get_batch(source, i):\n",
    "    '''\n",
    "    把数据进一步切分成长度为35的序列，最后返回的 data:[35, batch_size] ,每一列表示一个连续的序列\n",
    "    '''\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03a9a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n",
      "torch.Size([700])\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 0)\n",
    "print(data.size())\n",
    "print(targets.size()) # target 表示待预测的下一个正确的词，用于计算模型损失，进而更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ec3abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = ''\n",
    "for id in data[:, 0]:\n",
    "    src = src + (' %s' % TEXT.vocab.itos[id])\n",
    "    \n",
    "src.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd6f451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = ''\n",
    "for id in targets[0::20]:\n",
    "    tgt = tgt + (' %s' % TEXT.vocab.itos[id])\n",
    "    \n",
    "tgt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc440fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    给原始序列添加位置编码\n",
    "    '''\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 首先初始化为0\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # sine 和 cosine 来生成位置信息\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 词经过嵌入层后，再加上位置信息\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf57b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a0d67d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bd2ba48220>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c21619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    '''\n",
    "    ntoken: 词表大小，用于构建嵌入层\n",
    "    ninp: 模型维度\n",
    "    nhead: 多头注意力机制中 head 数目\n",
    "    nhid: 前馈神经网络的维度\n",
    "    nlayers: TransformerEncoderLayer叠加层数\n",
    "    '''\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # 位置编码\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) # EncoderLayer\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # Encoder\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # 嵌入层\n",
    "        self.ninp = ninp # 模型维度\n",
    "        \n",
    "        # decoder 用于将隐藏层的表示转化成词表中 token 的概率分布\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "    def forward(self, src):\n",
    "        # 生成 mask ，保证模型只能看到当前位置之前的信息\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src) # 位置编码\n",
    "        output = self.transformer_encoder(src, mask=self.src_mask, src_key_padding_mask=None)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4716c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4c0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "emsize = 200 # 嵌入层维度\n",
    "nhid = 200 # nn.TransformerEncoder 中前馈神经网络的维度\n",
    "nlayers = 2 # 编码器中 nn.TransformerEncoderLayer 层数\n",
    "nhead = 2 # 多头注意力机制中“头”的数目\n",
    "dropout = 0.2 # dropout\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5665d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier_normal_初始化参数\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_normal_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f4d0272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(28785, 200)\n",
       "  (decoder): Linear(in_features=200, out_features=28785, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf0ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率\n",
    "lr = 2.0\n",
    "# 随机梯度下降\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# 动态调整学习率\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52191dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train() # 训练模式，更新模型参数\n",
    "    total_loss = 0.\n",
    "    start_time = time.time() # 用于记录模型的训练时长\n",
    "    ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        # 获取批次数据\n",
    "        data, targets = get_batch(train_data, i) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 梯度裁剪，防止梯度消失/爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        # 优化参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印训练记录\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b719e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # 评估模式，不更新模型参数，仅评估模型当前的表现\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af6cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_loss = float(\"inf\")\n",
    "# epochs = 3 # 共训练3个epoch\n",
    "# best_model = None\n",
    "\n",
    "# # for epoch in range(1, epochs + 1): \n",
    "# for epoch in range(1, 2): # Kagging test\n",
    "#     epoch_start_time = time.time()\n",
    "#     # 训练过程\n",
    "#     train()\n",
    "#     # 验证过程\n",
    "#     val_loss = evaluate(model, val_data)\n",
    "#     # 打印验证结果\n",
    "#     print('-' * 89)\n",
    "#     print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "#           'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "#                                      val_loss, math.exp(val_loss)))\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     # 记录最佳模型\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = model\n",
    "    \n",
    "#     # 调整学习率\n",
    "#     scheduler.step()\n",
    "\n",
    "# # # 保存模型\n",
    "# # if not os.path.exists('datasets/models'):\n",
    "# #     os.makedirs('datasets/models')\n",
    "# # torch.save({'state_dict': model.state_dict()}, 'datasets/models/best_model.pth.tar')\n",
    "\n",
    "# # 保存模型\n",
    "# if not os.path.exists('temp/models'):\n",
    "#     os.makedirs('temp/models')\n",
    "# torch.save({'state_dict': model.state_dict()}, 'temp/models/best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86bc4b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 2.00 | ms/batch 44.59 | loss  7.50 | ppl  1809.86\n",
      "| epoch   1 |   400/ 2981 batches | lr 2.00 | ms/batch 44.22 | loss  6.68 | ppl   798.98\n",
      "| epoch   1 |   600/ 2981 batches | lr 2.00 | ms/batch 44.50 | loss  6.33 | ppl   563.60\n",
      "| epoch   1 |   800/ 2981 batches | lr 2.00 | ms/batch 44.43 | loss  6.19 | ppl   488.39\n",
      "| epoch   1 |  1000/ 2981 batches | lr 2.00 | ms/batch 44.37 | loss  6.04 | ppl   420.31\n",
      "| epoch   1 |  1200/ 2981 batches | lr 2.00 | ms/batch 44.26 | loss  6.01 | ppl   405.88\n",
      "| epoch   1 |  1400/ 2981 batches | lr 2.00 | ms/batch 44.76 | loss  5.93 | ppl   375.10\n",
      "| epoch   1 |  1600/ 2981 batches | lr 2.00 | ms/batch 44.82 | loss  5.92 | ppl   372.42\n",
      "| epoch   1 |  1800/ 2981 batches | lr 2.00 | ms/batch 44.39 | loss  5.82 | ppl   336.35\n",
      "| epoch   1 |  2000/ 2981 batches | lr 2.00 | ms/batch 44.45 | loss  5.80 | ppl   328.93\n",
      "| epoch   1 |  2200/ 2981 batches | lr 2.00 | ms/batch 44.39 | loss  5.68 | ppl   292.47\n",
      "| epoch   1 |  2400/ 2981 batches | lr 2.00 | ms/batch 44.43 | loss  5.72 | ppl   303.39\n",
      "| epoch   1 |  2600/ 2981 batches | lr 2.00 | ms/batch 44.32 | loss  5.71 | ppl   300.66\n",
      "| epoch   1 |  2800/ 2981 batches | lr 2.00 | ms/batch 44.49 | loss  5.60 | ppl   270.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 137.51s | valid loss  5.55 | valid ppl   257.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2981 batches | lr 1.80 | ms/batch 44.47 | loss  5.61 | ppl   271.91\n",
      "| epoch   2 |   400/ 2981 batches | lr 1.80 | ms/batch 44.32 | loss  5.59 | ppl   267.32\n",
      "| epoch   2 |   600/ 2981 batches | lr 1.80 | ms/batch 44.16 | loss  5.41 | ppl   223.79\n",
      "| epoch   2 |   800/ 2981 batches | lr 1.80 | ms/batch 44.35 | loss  5.44 | ppl   230.44\n",
      "| epoch   2 |  1000/ 2981 batches | lr 1.80 | ms/batch 44.29 | loss  5.37 | ppl   215.62\n",
      "| epoch   2 |  1200/ 2981 batches | lr 1.80 | ms/batch 44.23 | loss  5.39 | ppl   218.73\n",
      "| epoch   2 |  1400/ 2981 batches | lr 1.80 | ms/batch 44.67 | loss  5.39 | ppl   218.47\n",
      "| epoch   2 |  1600/ 2981 batches | lr 1.80 | ms/batch 44.81 | loss  5.42 | ppl   225.42\n",
      "| epoch   2 |  1800/ 2981 batches | lr 1.80 | ms/batch 44.57 | loss  5.33 | ppl   205.97\n",
      "| epoch   2 |  2000/ 2981 batches | lr 1.80 | ms/batch 44.47 | loss  5.35 | ppl   209.58\n",
      "| epoch   2 |  2200/ 2981 batches | lr 1.80 | ms/batch 44.52 | loss  5.22 | ppl   185.14\n",
      "| epoch   2 |  2400/ 2981 batches | lr 1.80 | ms/batch 44.51 | loss  5.29 | ppl   198.32\n",
      "| epoch   2 |  2600/ 2981 batches | lr 1.80 | ms/batch 44.44 | loss  5.30 | ppl   199.53\n",
      "| epoch   2 |  2800/ 2981 batches | lr 1.80 | ms/batch 44.48 | loss  5.21 | ppl   183.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 137.57s | valid loss  5.34 | valid ppl   209.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2981 batches | lr 1.71 | ms/batch 44.62 | loss  5.26 | ppl   191.62\n",
      "| epoch   3 |   400/ 2981 batches | lr 1.71 | ms/batch 44.38 | loss  5.28 | ppl   195.60\n",
      "| epoch   3 |   600/ 2981 batches | lr 1.71 | ms/batch 44.49 | loss  5.09 | ppl   161.63\n",
      "| epoch   3 |   800/ 2981 batches | lr 1.71 | ms/batch 44.54 | loss  5.14 | ppl   170.73\n",
      "| epoch   3 |  1000/ 2981 batches | lr 1.71 | ms/batch 44.56 | loss  5.09 | ppl   162.53\n",
      "| epoch   3 |  1200/ 2981 batches | lr 1.71 | ms/batch 44.71 | loss  5.12 | ppl   166.69\n",
      "| epoch   3 |  1400/ 2981 batches | lr 1.71 | ms/batch 44.56 | loss  5.13 | ppl   168.75\n",
      "| epoch   3 |  1600/ 2981 batches | lr 1.71 | ms/batch 44.47 | loss  5.18 | ppl   176.95\n",
      "| epoch   3 |  1800/ 2981 batches | lr 1.71 | ms/batch 44.49 | loss  5.09 | ppl   162.56\n",
      "| epoch   3 |  2000/ 2981 batches | lr 1.71 | ms/batch 44.62 | loss  5.11 | ppl   166.05\n",
      "| epoch   3 |  2200/ 2981 batches | lr 1.71 | ms/batch 44.47 | loss  4.98 | ppl   145.02\n",
      "| epoch   3 |  2400/ 2981 batches | lr 1.71 | ms/batch 44.46 | loss  5.05 | ppl   156.63\n",
      "| epoch   3 |  2600/ 2981 batches | lr 1.71 | ms/batch 44.38 | loss  5.07 | ppl   159.28\n",
      "| epoch   3 |  2800/ 2981 batches | lr 1.71 | ms/batch 44.38 | loss  4.99 | ppl   147.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 137.62s | valid loss  5.25 | valid ppl   190.87\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # 共训练3个epoch\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1): \n",
    "# for epoch in range(1, 2): # Kagging test\n",
    "    epoch_start_time = time.time()\n",
    "    # 训练过程\n",
    "    train()\n",
    "    # 验证过程\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    # 打印验证结果\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    \n",
    "    # 记录最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    \n",
    "    # 调整学习率\n",
    "    scheduler.step()\n",
    "\n",
    "# # 保存模型\n",
    "# if not os.path.exists('datasets/models'):\n",
    "#     os.makedirs('datasets/models')\n",
    "# torch.save({'state_dict': model.state_dict()}, 'datasets/models/best_model.pth.tar')\n",
    "\n",
    "# 保存模型\n",
    "if not os.path.exists('temp/models'):\n",
    "    os.makedirs('temp/models')\n",
    "torch.save({'state_dict': model.state_dict()}, 'temp/models/best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd02ce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "| End of training | test ppl   175.64\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 计算交叉熵损失\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "\n",
    "# 计算困惑度\n",
    "ppl = math.exp(test_loss)\n",
    "print('=' * 40)\n",
    "print('| End of training | test ppl {:8.2f}'.format(ppl))\n",
    "print('=' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d876f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先实例化一个模型\n",
    "model = TransformerModel(len(TEXT.vocab.stoi), ninp=200, nhead=2, nhid=200, nlayers=2, dropout=0.2).to(device)\n",
    "# 模型加载训练好的参数\n",
    "# checkpoint = torch.load('datasets/models/best_model.pth.tar')\n",
    "checkpoint = torch.load('temp/models/best_model.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb13af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已知序列\n",
    "history = 'it seems'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54eaa6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_11000/1640708594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "h = []\n",
    "for w in history.split():\n",
    "    h.append([TEXT.vocab.stoi[w]])\n",
    "\n",
    "while(True):\n",
    "    # 把列表转化成 tensor ，然后计算模型输出\n",
    "    output = model(torch.tensor(h).to(device))\n",
    "    # 获取概率最大的5个单词的 id\n",
    "    idxs = output[-1].argsort(descending=True).view(-1)[:10]\n",
    "    # 随机选择其中一个\n",
    "    r = random.randint(0, 10)\n",
    "    h.append([r])\n",
    "    # 句子结束\n",
    "    if TEXT.vocab.itos[r] == '.' or TEXT.vocab.itos[r] == '<eos>':\n",
    "        break\n",
    "\n",
    "# 将下标转化成句子\n",
    "sent = ''\n",
    "for w in h:\n",
    "    sent += TEXT.vocab.itos[w[0]] + ' '\n",
    "\n",
    "# out_path = './tmp/hypotheses.txt'\n",
    "out_path = './temp/hypotheses.txt'\n",
    "# out_path = './submit/hypotheses.txt'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('history: ' + history + '\\n')\n",
    "    f.write('hypotheses: ' + sent + '\\n')\n",
    "    \n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebbcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a5e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
